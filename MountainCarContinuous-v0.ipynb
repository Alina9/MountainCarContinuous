{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import make\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size, 32)\n",
    "        self.linear2 = nn.Linear(32, 32)\n",
    "        self.linear3 = nn.Linear(32, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.linear1.weight.data.normal_(0, 1e-1)\n",
    "        self.linear2.weight.data.normal_(0, 1e-1)\n",
    "        self.linear3.weight.data.normal_(0, 1e-2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size + action_size, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.linear1.weight.data.normal_(0, 1e-1)\n",
    "        self.linear2.weight.data.normal_(0, 1e-1)\n",
    "        self.linear3.weight.data.normal_(0, 1e-2)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=1)        \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, element):\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = element\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self):\n",
    "        return list(zip(*random.sample(self.memory, self.batch_size)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, gamma, tau):\n",
    "         # Actor Network and Target Network\n",
    "        self.actor = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        # Critic Network and Target Network\n",
    "        self.critic = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        # copy weights\n",
    "        self.hard_update(self.actor_target, self.actor)\n",
    "        self.hard_update(self.critic_target, self.critic)\n",
    "        \n",
    "        self.memory = Memory(buffer_size, batch_size)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.sd = 1\n",
    "        \n",
    "    def hard_update(self, target, network):\n",
    "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def soft_update(self, target, network):\n",
    "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data + (1-self.tau)*target_param.data)\n",
    "            \n",
    "    def learn(self, batch):\n",
    "        \n",
    "        state, action, reward, next_state, done = batch\n",
    "\n",
    "        state = torch.tensor(state).to(device).float()\n",
    "        next_state = torch.tensor(next_state).to(device).float()\n",
    "        reward = torch.tensor(reward).to(device).float()\n",
    "        action = torch.tensor(action).to(device)\n",
    "        done = torch.tensor(done).to(device).int()\n",
    "        \n",
    "        # update critic\n",
    "        next_action = self.actor_target(next_state)\n",
    "\n",
    "        Q_target = self.critic_target(next_state, next_action).detach()\n",
    "        Q_target = reward.unsqueeze(1) + (self.gamma*Q_target*((1-done).unsqueeze(1)))\n",
    "\n",
    "        \n",
    "        critic_loss = F.mse_loss(self.critic(state, action), Q_target)        \n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # update actor\n",
    "        \n",
    "        action_prediction = self.actor(state)\n",
    "        actor_loss = -self.critic(state, action_prediction).mean()\n",
    "        \n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # update actor_target and critic_target\n",
    "        \n",
    "        self.soft_update(self.critic_target, self.critic)\n",
    "        self.soft_update(self.actor_target, self.actor)\n",
    "        \n",
    "    def act(self, state, noise = True):\n",
    "        state =  torch.tensor(state).to(device).float()\n",
    "        action = self.actor(state).cpu().data.numpy()\n",
    "        \n",
    "        if noise:\n",
    "            noise = np.random.normal(0, self.sd)\n",
    "            action = action + noise\n",
    "        \n",
    "        if action[0] > 1:\n",
    "            action[0] = 1\n",
    "        if action[0] < -1:\n",
    "            action[0] = -1\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        if len(self.memory) >= self.memory.batch_size:\n",
    "            self.learn(self.memory.sample())\n",
    "        \n",
    "    def save(self):\n",
    "        torch.save(self.actor, \"actor.pkl\")\n",
    "        torch.save(self.critic, \"critic.pkl\")\n",
    "        \n",
    "    def test(self):\n",
    "        new_env = make(\"MountainCarContinuous-v0\")\n",
    "        new_env.seed(9)\n",
    "        reward = []\n",
    "        for i in range(50):\n",
    "            state = new_env.reset()\n",
    "            local_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state, noise = False)\n",
    "                state, r, done, _ = new_env.step(action)\n",
    "                local_reward += r\n",
    "            reward.append(local_reward)\n",
    "        return reward\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of eche action = 1\n",
      "size of state = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alina\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "np.random.seed(9)\n",
    "env.seed(9)\n",
    "\n",
    "action_size = env.action_space.shape[0]\n",
    "print(f'size of eche action = {action_size}')\n",
    "state_size = env.observation_space.shape[0]\n",
    "print(f'size of state = {state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99            \n",
    "TAU = 1e-3                    \n",
    "EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1, current reward: -53.14649849925293\n",
      "episode: 2, current reward: -51.18537799228434\n",
      "episode: 3, current reward: -47.6784317369832\n",
      "episode: 4, current reward: -50.41504956254138\n",
      "episode: 5, current reward: -50.20164968504307\n",
      "episode: 6, current reward: -48.04421274303699\n",
      "episode: 7, current reward: -49.10299902205487\n",
      "episode: 8, current reward: -47.90219743382987\n",
      "episode: 9, current reward: -45.90659546515292\n",
      "episode: 10, current reward: 70.31940161171292, max reward: -0.1671011956508624, mean reward: -0.1675511717809639\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alina\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Actor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Alina\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Alina\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Critic. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11, current reward: -45.481127945608485\n",
      "episode: 12, current reward: -46.77662850898423\n",
      "episode: 13, current reward: -46.07504396321992\n",
      "episode: 14, current reward: -46.70766253888688\n",
      "episode: 15, current reward: -45.73336123741364\n",
      "episode: 16, current reward: -44.46730489070606\n",
      "episode: 17, current reward: -45.18101737811812\n",
      "episode: 18, current reward: -44.54450868555374\n",
      "episode: 19, current reward: -42.575676645489615\n",
      "episode: 20, current reward: -42.66587927884428\n",
      "episode: 21, current reward: -42.07691309894329\n",
      "episode: 22, current reward: -40.610491788458255\n",
      "episode: 23, current reward: -41.81188579535066\n",
      "episode: 24, current reward: -43.177421322604765\n",
      "episode: 25, current reward: -40.12162912171294\n",
      "episode: 26, current reward: -39.050530328081834\n",
      "episode: 27, current reward: -39.88279051708597\n",
      "episode: 28, current reward: -40.3046859177759\n",
      "episode: 29, current reward: -39.71056518473935\n",
      "episode: 30, current reward: -42.096173301496364\n",
      "episode: 31, current reward: -39.29619863468082\n",
      "episode: 32, current reward: -39.47257449178069\n",
      "episode: 33, current reward: -38.38922651389443\n",
      "episode: 34, current reward: -38.10924331960344\n",
      "episode: 35, current reward: -37.090116779651034\n",
      "episode: 36, current reward: -35.77099323761636\n",
      "episode: 37, current reward: -36.722160122487594\n",
      "episode: 38, current reward: -33.65006050991409\n",
      "episode: 39, current reward: -34.07812318075936\n",
      "episode: 40, current reward: -34.83322831019854\n",
      "episode: 41, current reward: -35.00923884375953\n",
      "episode: 42, current reward: -34.158288582686765\n",
      "episode: 43, current reward: -32.053713407527674\n",
      "episode: 44, current reward: -31.947183254021468\n",
      "episode: 45, current reward: -30.28896895941251\n",
      "episode: 46, current reward: 82.9045513595903, max reward: 96.50018992308404, mean reward: 80.52535763590274\n",
      "Saved\n",
      "episode: 47, current reward: 73.0382509039508, max reward: 96.69772315613997, mean reward: 81.33923399584957\n",
      "Saved\n",
      "episode: 48, current reward: 91.6343115231962, max reward: 97.25637993267925, mean reward: 78.79185966828595\n",
      "episode: 49, current reward: 81.45696759434, max reward: 96.84826402427356, mean reward: 87.62067739616063\n",
      "Saved\n",
      "episode: 50, current reward: 79.41307034191678, max reward: 97.31925290519605, mean reward: 92.6964812547993\n",
      "Saved\n",
      "episode: 51, current reward: 91.83994291401754, max reward: 97.22435457223206, mean reward: 85.0835901369276\n",
      "episode: 52, current reward: 85.4966483226517, max reward: 96.65367911087077, mean reward: 91.72890968652277\n",
      "episode: 53, current reward: 89.24984913131539, max reward: 96.26409091151528, mean reward: 95.56740360436916\n",
      "Saved\n",
      "episode: 54, current reward: 88.09263413054157, max reward: 96.54290954734361, mean reward: 91.70642082912313\n",
      "episode: 55, current reward: 89.74876778380391, max reward: 97.10680717592196, mean reward: 92.69408823989414\n",
      "episode: 56, current reward: 93.22866324949791, max reward: 96.3214250575872, mean reward: 95.56317708458702\n",
      "episode: 57, current reward: 91.968967577325, max reward: 95.89409591591958, mean reward: 94.93512401615386\n",
      "episode: 58, current reward: 87.08796012255786, max reward: 96.1566893255225, mean reward: 91.09688752185266\n",
      "episode: 59, current reward: 94.17836920393049, max reward: 96.44205728596309, mean reward: 93.8643153087341\n",
      "episode: 60, current reward: 91.06380991937284, max reward: 96.19165463199819, mean reward: 91.26463978538773\n",
      "episode: 61, current reward: 89.91760275985646, max reward: 94.70863246572794, mean reward: 92.36746549366816\n",
      "episode: 62, current reward: 92.56874018574916, max reward: 95.54026071953423, mean reward: 86.6399177689048\n",
      "episode: 63, current reward: 92.85752079316586, max reward: 95.78912865990704, mean reward: 89.19095440809178\n",
      "episode: 64, current reward: 91.94375343724757, max reward: 95.17208914626313, mean reward: 90.15143535979783\n",
      "episode: 65, current reward: 92.81620077799323, max reward: 94.5611633183182, mean reward: 93.75705177755526\n",
      "episode: 66, current reward: 91.36129619604722, max reward: 95.40209224418224, mean reward: 93.82815352598783\n",
      "episode: 67, current reward: 92.25088944369736, max reward: 95.08565221666554, mean reward: 94.07404920342884\n",
      "episode: 68, current reward: 94.7208034061382, max reward: 95.64736300890296, mean reward: 92.35960912847689\n",
      "episode: 69, current reward: 95.0399273993412, max reward: 95.86366805997017, mean reward: 90.99155355436135\n",
      "episode: 70, current reward: 93.80870546942458, max reward: 95.54029165346645, mean reward: 90.69025128216786\n",
      "episode: 71, current reward: 94.13520812918773, max reward: 94.8514570536314, mean reward: 93.80584938234271\n",
      "episode: 72, current reward: 94.81462436767428, max reward: 96.17723833553991, mean reward: 91.59048783524813\n",
      "episode: 73, current reward: 94.22180521462418, max reward: 95.82368030219881, mean reward: 92.2438637743338\n",
      "episode: 74, current reward: 94.36901061989052, max reward: 95.81373459140536, mean reward: 90.52985545461064\n",
      "episode: 75, current reward: 94.59035605303197, max reward: 95.41541650473566, mean reward: 93.80229122480348\n",
      "episode: 76, current reward: 94.92995022217076, max reward: 95.57850599584991, mean reward: 93.86037929108747\n",
      "episode: 77, current reward: 91.94000096492873, max reward: 95.5351303834293, mean reward: 93.81132375504312\n",
      "episode: 78, current reward: 92.9332101841871, max reward: 95.42768239896853, mean reward: 94.56530585573435\n",
      "episode: 79, current reward: 93.7008619601239, max reward: 95.77253179000736, mean reward: 94.10885838415943\n",
      "episode: 80, current reward: 95.50140490078184, max reward: 94.73104796634642, mean reward: 92.83577193227126\n",
      "episode: 81, current reward: 94.82051388062686, max reward: 95.71535430236528, mean reward: 94.4266842477376\n",
      "episode: 82, current reward: 93.60319666064234, max reward: 95.43569418572655, mean reward: 94.0801440454977\n",
      "episode: 83, current reward: 95.53068833246037, max reward: 95.51286972095427, mean reward: 93.90002322327481\n",
      "episode: 84, current reward: 94.9252385822249, max reward: 95.76520578952497, mean reward: 93.72474501812063\n",
      "episode: 85, current reward: 94.37315436611162, max reward: 95.57235874246268, mean reward: 90.96436585780117\n",
      "episode: 86, current reward: 91.80518051517802, max reward: 95.49956448601063, mean reward: 90.16534944116466\n",
      "episode: 87, current reward: 93.41931622044604, max reward: 95.42347848945121, mean reward: 91.6866977524815\n",
      "episode: 88, current reward: 88.64713977982086, max reward: 95.30583556130853, mean reward: 91.36461495985803\n",
      "episode: 89, current reward: 89.37595759954918, max reward: 95.59101768565647, mean reward: 90.184026493678\n",
      "episode: 90, current reward: 84.14520815093566, max reward: 95.5417801544349, mean reward: 93.16038782737591\n",
      "episode: 91, current reward: 94.17249182824467, max reward: 95.41292630252588, mean reward: 93.14945412315952\n",
      "episode: 92, current reward: 88.46800666288027, max reward: 95.50234405268267, mean reward: 93.06640832895899\n",
      "episode: 93, current reward: 94.85399119473198, max reward: 95.26517445251547, mean reward: 94.15704268239826\n",
      "episode: 94, current reward: 94.47461284879319, max reward: 95.04909825552366, mean reward: 94.57294654282462\n",
      "episode: 95, current reward: 95.02007554146886, max reward: 95.08547876509438, mean reward: 94.28205980638403\n",
      "episode: 96, current reward: 95.08362858945497, max reward: 94.95380198976153, mean reward: 94.34497179079294\n",
      "episode: 97, current reward: 95.21527375691706, max reward: 95.02523493051528, mean reward: 94.57828060173902\n",
      "episode: 98, current reward: 92.18957276856507, max reward: 94.80873045605796, mean reward: 94.48189948664408\n",
      "episode: 99, current reward: 94.93630238009915, max reward: 94.86471023695033, mean reward: 94.51114380531958\n",
      "episode: 100, current reward: 94.95501017893078, max reward: 95.12412467004384, mean reward: 94.4444973682586\n",
      "episode: 101, current reward: 94.86130598478155, max reward: 94.92512784751939, mean reward: 94.51765706930559\n",
      "episode: 102, current reward: 94.56541248321187, max reward: 95.16216255635416, mean reward: 94.26084605186415\n",
      "episode: 103, current reward: 94.85737796883758, max reward: 95.08138284123844, mean reward: 94.57017931875778\n",
      "episode: 104, current reward: 94.6125800753638, max reward: 95.21630717888627, mean reward: 93.86814599545785\n",
      "episode: 105, current reward: 94.43396364685478, max reward: 95.2816405023521, mean reward: 94.04814388858983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 106, current reward: 93.85514439342722, max reward: 95.04757696229204, mean reward: 94.52736844128205\n",
      "episode: 107, current reward: 95.18805179756325, max reward: 95.08555149768864, mean reward: 94.30867155191494\n",
      "episode: 108, current reward: 95.00574102825938, max reward: 95.09466500597135, mean reward: 94.54465952061629\n",
      "episode: 109, current reward: 93.7933504665332, max reward: 94.97931338055065, mean reward: 94.45459951254335\n",
      "episode: 110, current reward: 94.59527192049231, max reward: 94.82623057922397, mean reward: 94.50218744144335\n",
      "episode: 111, current reward: 94.79850622844322, max reward: 95.12354240943755, mean reward: 93.80464740737442\n",
      "episode: 112, current reward: 92.63657588156033, max reward: 94.91657993003935, mean reward: 94.54551991375145\n",
      "episode: 113, current reward: 94.63586067059025, max reward: 94.95865358785576, mean reward: 94.54322213763959\n",
      "episode: 114, current reward: 94.82679571721349, max reward: 95.16155175880314, mean reward: 94.48725319027352\n",
      "episode: 115, current reward: 95.36979070283816, max reward: 95.37315539711472, mean reward: 93.68651116686789\n",
      "episode: 116, current reward: 93.42224307269947, max reward: 95.25832677904076, mean reward: 93.91650227219081\n",
      "episode: 117, current reward: 94.6732355276646, max reward: 95.2317580726327, mean reward: 94.23534172978478\n",
      "episode: 118, current reward: 94.92130439695359, max reward: 95.2519285915697, mean reward: 94.15652420658228\n",
      "episode: 119, current reward: 88.1371351224918, max reward: 95.16202866770004, mean reward: 93.85124302249095\n",
      "episode: 120, current reward: 95.0328844741362, max reward: 95.27574277805674, mean reward: 94.08904540953061\n",
      "episode: 121, current reward: 89.67423188658091, max reward: 95.10022872005565, mean reward: 94.54130397009956\n",
      "episode: 122, current reward: 88.99631101670684, max reward: 95.02817009995125, mean reward: 94.05085926880814\n",
      "episode: 123, current reward: 95.25645485129799, max reward: 95.16048681797542, mean reward: 94.5801850146302\n",
      "episode: 124, current reward: 94.93195880007066, max reward: 94.92700784428011, mean reward: 94.46961661103322\n",
      "episode: 125, current reward: 94.38570560599857, max reward: 95.10566627126526, mean reward: 94.56856610557185\n",
      "episode: 126, current reward: 93.93254835545798, max reward: 95.06678533390239, mean reward: 94.5410888823904\n",
      "episode: 127, current reward: 94.31459131018708, max reward: 94.90522549696519, mean reward: 94.51830922197252\n",
      "episode: 128, current reward: 94.20303283545073, max reward: 94.86065964488442, mean reward: 94.50475793750572\n",
      "episode: 129, current reward: 93.93626356510379, max reward: 94.74822855305335, mean reward: 94.41057441417334\n",
      "episode: 130, current reward: 94.8900245789338, max reward: 95.03240196874974, mean reward: 94.08620915663518\n",
      "episode: 131, current reward: 94.6987955405339, max reward: 94.83154963569928, mean reward: 94.50993192649177\n",
      "episode: 132, current reward: 94.87167044795846, max reward: 94.69810426131082, mean reward: 94.29894125562122\n",
      "episode: 133, current reward: 94.62055241695506, max reward: 94.71850802021936, mean reward: 94.43485521979663\n",
      "episode: 134, current reward: 94.70839160952157, max reward: 94.71538415324252, mean reward: 94.42605936259167\n",
      "episode: 135, current reward: 94.35194280830021, max reward: 94.85783635591825, mean reward: 94.4900733185459\n",
      "episode: 136, current reward: 95.12372341265578, max reward: 94.89629888167262, mean reward: 94.47726859572012\n",
      "episode: 137, current reward: 94.9479853967114, max reward: 95.1057847087219, mean reward: 93.60760699788253\n",
      "episode: 138, current reward: 95.13646869525238, max reward: 94.94290903703326, mean reward: 94.49058258555083\n",
      "episode: 139, current reward: 94.9989829494089, max reward: 94.84101829773397, mean reward: 94.48047304362564\n",
      "episode: 140, current reward: 94.77554821527055, max reward: 95.12810151820415, mean reward: 94.02453930865349\n",
      "episode: 141, current reward: 94.2265907708593, max reward: 95.07482354200931, mean reward: 94.40795009107967\n",
      "episode: 142, current reward: 94.77351945941918, max reward: 95.0584288207829, mean reward: 94.52675485024045\n",
      "episode: 143, current reward: 94.98239157962324, max reward: 94.83029012364041, mean reward: 94.45379727361738\n",
      "episode: 144, current reward: 94.52269790156961, max reward: 95.0935364763072, mean reward: 94.5875328211527\n",
      "episode: 145, current reward: 94.76667228716528, max reward: 94.82684529270674, mean reward: 94.44408597133561\n",
      "episode: 146, current reward: 95.04965809987304, max reward: 94.94291698700398, mean reward: 94.53266606944966\n",
      "episode: 147, current reward: 94.80193208372712, max reward: 95.15829858413221, mean reward: 94.56021493747225\n",
      "episode: 148, current reward: 93.90748416132479, max reward: 95.03064582228967, mean reward: 93.34024926099795\n",
      "episode: 149, current reward: 94.27695093881069, max reward: 94.95231543996209, mean reward: 94.1688022214576\n",
      "episode: 150, current reward: 94.71823288400985, max reward: 95.07171084586308, mean reward: 94.59986057493353\n",
      "episode: 151, current reward: 94.56501415538965, max reward: 95.29648072132893, mean reward: 94.22109119313906\n",
      "episode: 152, current reward: 95.18822856885966, max reward: 95.18368481563643, mean reward: 92.751595209965\n",
      "episode: 153, current reward: 94.24693321995338, max reward: 95.18064765855729, mean reward: 94.25531201929005\n",
      "episode: 154, current reward: 93.28783970292127, max reward: 95.30273764933844, mean reward: 93.62470107023236\n",
      "episode: 155, current reward: 93.81611992128833, max reward: 94.84767457409473, mean reward: 91.17052646637636\n",
      "episode: 156, current reward: 89.35646411181476, max reward: 95.22355591249325, mean reward: 94.19433637415486\n",
      "episode: 157, current reward: 92.12111162851474, max reward: 95.2124299782504, mean reward: 94.46081892358708\n",
      "episode: 158, current reward: 95.29611107315561, max reward: 95.09649871815468, mean reward: 94.55688102296892\n",
      "episode: 159, current reward: 95.1589837570072, max reward: 95.14539190051673, mean reward: 94.49647213511554\n",
      "episode: 160, current reward: 92.96795728576132, max reward: 94.87751260537398, mean reward: 94.38561259004344\n",
      "episode: 161, current reward: 94.62525453654847, max reward: 95.13920605773855, mean reward: 94.4257791004774\n",
      "episode: 162, current reward: 94.97702803911659, max reward: 94.40983865308662, mean reward: 89.50368940962349\n",
      "episode: 163, current reward: 90.89582748124141, max reward: 94.47305563036515, mean reward: 79.52779497672354\n",
      "episode: 164, current reward: 91.21885765438455, max reward: 93.77840191236074, mean reward: 89.72811033664426\n",
      "episode: 165, current reward: 89.50504501156166, max reward: 95.22171813879734, mean reward: 93.9342792709586\n",
      "episode: 166, current reward: 92.88801155609973, max reward: 93.40798707123967, mean reward: 29.876922611870967\n",
      "episode: 167, current reward: 90.21469823507236, max reward: 94.84142991789618, mean reward: 92.08359859178964\n",
      "episode: 168, current reward: 93.8029725850535, max reward: 94.31309451860075, mean reward: 90.42539368395228\n",
      "episode: 169, current reward: 88.3473536989485, max reward: 95.14915420404743, mean reward: 93.23149016702581\n",
      "episode: 170, current reward: 87.29752088616144, max reward: 94.2671094304326, mean reward: 90.34050603410189\n",
      "episode: 171, current reward: 88.17063084569216, max reward: 92.63377045621993, mean reward: 39.878682381007415\n",
      "episode: 172, current reward: 90.89666160189302, max reward: 92.32892765092511, mean reward: 34.327049081279526\n",
      "episode: 173, current reward: 92.26719703436058, max reward: 93.97795784440049, mean reward: 75.17709958371994\n",
      "episode: 174, current reward: 92.00939871500337, max reward: 92.40822476665696, mean reward: 40.19371417721488\n",
      "episode: 175, current reward: 92.64276592870152, max reward: 92.77571324222953, mean reward: 32.766074123592574\n",
      "episode: 176, current reward: 92.72995445794815, max reward: 92.50170433026241, mean reward: 67.06067090632496\n",
      "episode: 177, current reward: 89.87270363017542, max reward: 94.94473042431328, mean reward: 93.97951041339478\n",
      "episode: 178, current reward: 95.04094426090475, max reward: 94.82775993185282, mean reward: 93.57126593882558\n",
      "episode: 179, current reward: 94.0608560124287, max reward: 94.69741362987405, mean reward: 94.16108031610769\n",
      "episode: 180, current reward: 93.85356011555038, max reward: 94.47714722636809, mean reward: 92.70991103075266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 181, current reward: 91.97057642355544, max reward: 94.45282251206905, mean reward: 93.89331885798023\n",
      "episode: 182, current reward: 91.9001571718646, max reward: 94.4352181365683, mean reward: 93.94289190542477\n",
      "episode: 183, current reward: 94.01373760064558, max reward: 94.45463641070359, mean reward: 93.8295784164128\n",
      "episode: 184, current reward: 94.22424138737885, max reward: 94.54140757414393, mean reward: 94.14822240887015\n",
      "episode: 185, current reward: 94.8476464099151, max reward: 94.66796386859554, mean reward: 94.07759357872365\n",
      "episode: 186, current reward: 93.90717124471107, max reward: 94.68394847314511, mean reward: 93.76187104994398\n",
      "episode: 187, current reward: 94.44970678640136, max reward: 94.47507941303972, mean reward: 93.69156374803931\n",
      "episode: 188, current reward: 94.60687568673436, max reward: 94.4385216557855, mean reward: 93.41940868202924\n",
      "episode: 189, current reward: 91.76909100192341, max reward: 94.4760345790131, mean reward: 93.43999936871232\n",
      "episode: 190, current reward: 91.82405991960216, max reward: 94.64815895852946, mean reward: 93.41985217944224\n",
      "episode: 191, current reward: 95.02325071656819, max reward: 94.49114570288125, mean reward: 93.4133952925018\n",
      "episode: 192, current reward: 94.7601448294583, max reward: 94.45961153697459, mean reward: 93.8761896177711\n",
      "episode: 193, current reward: 93.01576513685808, max reward: 94.54351361719533, mean reward: 93.41813451869021\n",
      "episode: 194, current reward: 94.78273758836589, max reward: 94.70506624322229, mean reward: 93.371853409405\n",
      "episode: 195, current reward: 91.78987344102042, max reward: 94.77543972646194, mean reward: 92.02480433824971\n",
      "episode: 196, current reward: 91.01339456413645, max reward: 92.32824255979376, mean reward: 44.97001717119727\n",
      "episode: 197, current reward: 91.56633938801943, max reward: 91.76538967080864, mean reward: 56.14980264942288\n",
      "episode: 198, current reward: 91.85373117011994, max reward: 92.01233656063457, mean reward: 33.54974932742519\n",
      "episode: 199, current reward: 88.96635772993974, max reward: 92.15034761719153, mean reward: 34.805201579583084\n",
      "episode: 200, current reward: 87.8932664106997, max reward: 91.98271731318191, mean reward: 35.542231226603896\n",
      "episode: 201, current reward: 90.9571684074863, max reward: 91.8433139195979, mean reward: 87.01674570121148\n",
      "episode: 202, current reward: 91.76492226581955, max reward: 92.03097967572722, mean reward: 90.78755101553595\n",
      "episode: 203, current reward: 90.95304942820574, max reward: 91.89800905451261, mean reward: 55.36581365825359\n",
      "episode: 204, current reward: 92.01787089133713, max reward: 91.69490228842841, mean reward: 63.86701563518519\n",
      "episode: 205, current reward: 90.51880509856494, max reward: 91.8160601526815, mean reward: 88.84273605531311\n",
      "episode: 206, current reward: 91.09277979140296, max reward: 91.43711069844966, mean reward: 81.19962440255193\n",
      "episode: 207, current reward: 90.32879633610305, max reward: 91.60295824974361, mean reward: 84.4844342877276\n",
      "episode: 208, current reward: 90.85963997157731, max reward: 91.4739898774106, mean reward: 65.47327740037892\n",
      "episode: 209, current reward: 91.48064463287265, max reward: 91.21073163791144, mean reward: 71.52019857646229\n",
      "episode: 210, current reward: 91.32946971177111, max reward: 91.29694511613235, mean reward: 86.62269381490998\n",
      "episode: 211, current reward: 90.80719644238766, max reward: 94.06086127226904, mean reward: 90.08203360887148\n",
      "episode: 212, current reward: 90.11734942383379, max reward: 94.27417588965221, mean reward: 91.6237338216195\n",
      "episode: 213, current reward: 91.0094969037305, max reward: 94.16925519361718, mean reward: 90.24191768493534\n",
      "episode: 214, current reward: 89.2199358594661, max reward: 94.16523726125182, mean reward: 91.0489025708659\n",
      "episode: 215, current reward: 88.73034508748012, max reward: 94.01040298696675, mean reward: 90.57384607461523\n",
      "episode: 216, current reward: 89.20829678542219, max reward: 94.05579621304071, mean reward: 90.50639697290879\n",
      "episode: 217, current reward: 90.7484530106837, max reward: 94.12292032109325, mean reward: 90.35116872538147\n",
      "episode: 218, current reward: 91.14419707106578, max reward: 94.10647692463162, mean reward: 90.64043914083653\n",
      "episode: 219, current reward: 90.88262593257198, max reward: 93.97737954683457, mean reward: 90.2955348962066\n",
      "episode: 220, current reward: 89.06990300038198, max reward: 94.042293493785, mean reward: 90.89135643070802\n",
      "episode: 221, current reward: 90.69183890509005, max reward: 94.0286581467982, mean reward: 90.75247400750612\n",
      "episode: 222, current reward: 94.18806500044633, max reward: 93.97847197868954, mean reward: 90.55395224242805\n",
      "episode: 223, current reward: 90.63161271203359, max reward: 93.98416837311802, mean reward: 90.76940784799895\n",
      "episode: 224, current reward: 90.6378155447714, max reward: 93.96214859826512, mean reward: 90.82649123872143\n",
      "episode: 225, current reward: 93.90351266057986, max reward: 93.91737997156682, mean reward: 90.99480391891146\n",
      "episode: 226, current reward: 94.11818262674318, max reward: 93.91055612972714, mean reward: 90.62721700414914\n",
      "episode: 227, current reward: 93.57252609310645, max reward: 93.90550979473163, mean reward: 91.17129718966311\n",
      "episode: 228, current reward: 92.39384449024553, max reward: 93.89679906101232, mean reward: 90.46416763276565\n",
      "episode: 229, current reward: 84.28691040862773, max reward: 93.76029361954352, mean reward: 90.91518357690778\n",
      "episode: 230, current reward: 93.5494905028191, max reward: 93.77039990310672, mean reward: 90.42085079260737\n",
      "episode: 231, current reward: 90.66889999616296, max reward: 93.79824633567587, mean reward: 90.46127509542727\n",
      "episode: 232, current reward: 90.73072731579083, max reward: 93.80949396002563, mean reward: 90.70308958204652\n",
      "episode: 233, current reward: 88.1912390961743, max reward: 93.80496377307014, mean reward: 90.52603225878151\n",
      "episode: 234, current reward: 90.58354284179894, max reward: 93.23776477788134, mean reward: 90.30780849127775\n",
      "episode: 235, current reward: 90.55050482061516, max reward: 93.95409923850677, mean reward: 90.25148920658609\n",
      "episode: 236, current reward: 90.5349500908549, max reward: 93.82337813892576, mean reward: 90.519686643487\n",
      "episode: 237, current reward: 90.38915884936517, max reward: 90.39488569613324, mean reward: 89.93616880648092\n",
      "episode: 238, current reward: 90.70722875882805, max reward: 93.58837243632092, mean reward: 90.14447897833674\n",
      "episode: 239, current reward: 90.89853840572475, max reward: 93.68866865402428, mean reward: 90.4655909826035\n",
      "episode: 240, current reward: 87.040491923724, max reward: 93.23463123316442, mean reward: 90.25955917576601\n",
      "episode: 241, current reward: 91.02236220310499, max reward: 93.72529179572838, mean reward: 90.36061780918816\n",
      "episode: 242, current reward: 90.59977601254836, max reward: 93.71209662754455, mean reward: 90.41089220258195\n",
      "episode: 243, current reward: 91.33196951801006, max reward: 93.7468562499191, mean reward: 90.78823201519175\n",
      "episode: 244, current reward: 90.69070962118066, max reward: 93.67934377442873, mean reward: 90.34411632579128\n",
      "episode: 245, current reward: 93.34316120028843, max reward: 93.56498835839515, mean reward: 90.23047082699487\n",
      "episode: 246, current reward: 93.26285649301778, max reward: 93.53426342488879, mean reward: 90.16682174958723\n",
      "episode: 247, current reward: 90.56788024458643, max reward: 93.55738181381771, mean reward: 90.40355737794252\n",
      "episode: 248, current reward: 93.6791323201428, max reward: 93.588923907021, mean reward: 90.62650422411012\n",
      "episode: 249, current reward: 93.6313047192235, max reward: 93.52548863466143, mean reward: 90.41753964517687\n",
      "episode: 250, current reward: 92.94153969146461, max reward: 93.53831106373471, mean reward: 90.2421487346465\n",
      "episode: 251, current reward: 93.57708905465968, max reward: 90.40360676943914, mean reward: 89.90265126122573\n",
      "episode: 252, current reward: 90.77455268360013, max reward: 93.52073700763249, mean reward: 90.56010374668884\n",
      "episode: 253, current reward: 93.6853910055207, max reward: 93.57942053498017, mean reward: 90.25386475834078\n",
      "episode: 254, current reward: 93.73394168948474, max reward: 93.59784194524558, mean reward: 90.71895502646252\n",
      "episode: 255, current reward: 90.51610821328063, max reward: 93.30352374454289, mean reward: 90.26803272076837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 256, current reward: 92.48726180031379, max reward: 93.4768340863681, mean reward: 90.65969965393933\n",
      "episode: 257, current reward: 90.35226023674224, max reward: 93.53932694260972, mean reward: 90.56801656469668\n",
      "episode: 258, current reward: 87.38243606931096, max reward: 93.4529083644023, mean reward: 90.57776021042699\n",
      "episode: 259, current reward: 90.83738289941954, max reward: 93.41240254877964, mean reward: 90.1688160644587\n",
      "episode: 260, current reward: 90.97066878478256, max reward: 93.49524083793462, mean reward: 90.46840736043558\n",
      "episode: 261, current reward: 89.61737385118477, max reward: 93.42717788145171, mean reward: 90.64614797055819\n",
      "episode: 262, current reward: 90.27650887202225, max reward: 93.36126421297972, mean reward: 89.99512471867854\n",
      "episode: 263, current reward: 90.67945416558167, max reward: 93.35437310288285, mean reward: 90.45573427592382\n",
      "episode: 264, current reward: 90.38481250918122, max reward: 93.4104235547272, mean reward: 90.06934698449112\n",
      "episode: 265, current reward: 90.42539618280341, max reward: 93.19298207973357, mean reward: 89.89917673303637\n",
      "episode: 266, current reward: 91.61119220205626, max reward: 93.28516625249404, mean reward: 89.91170232072422\n",
      "episode: 267, current reward: 90.4596123031232, max reward: 93.41814021092776, mean reward: 90.21132475954244\n",
      "episode: 268, current reward: 90.86435133695227, max reward: 93.36385067894206, mean reward: 90.09079875065675\n",
      "episode: 269, current reward: 90.75511984760765, max reward: 93.40867253766179, mean reward: 90.44172812283463\n",
      "episode: 270, current reward: 90.3249580777669, max reward: 93.33844564062076, mean reward: 90.0207602457117\n",
      "episode: 271, current reward: 90.40505756900988, max reward: 93.26867118589094, mean reward: 90.20935063059666\n",
      "episode: 272, current reward: 90.64168926361621, max reward: 93.27577563326126, mean reward: 89.95756228153512\n",
      "episode: 273, current reward: 90.71289628708604, max reward: 93.169858057796, mean reward: 90.19413628933349\n",
      "episode: 274, current reward: 90.30614124298039, max reward: 93.22053870980783, mean reward: 89.86553759560282\n",
      "episode: 275, current reward: 90.41549713784502, max reward: 93.2713964071788, mean reward: 90.27100228342182\n",
      "episode: 276, current reward: 93.41405406338588, max reward: 93.3020745850632, mean reward: 90.48345843761676\n",
      "episode: 277, current reward: 90.86104694664128, max reward: 93.33125598007399, mean reward: 89.91311351894808\n",
      "episode: 278, current reward: 90.3481980775133, max reward: 93.07617289908447, mean reward: 89.84487390094559\n",
      "episode: 279, current reward: 90.23735376230375, max reward: 93.16697687363356, mean reward: 89.79401147970997\n",
      "episode: 280, current reward: 90.49216018077846, max reward: 93.15721941626668, mean reward: 89.84743293517016\n",
      "episode: 281, current reward: 90.63608833030267, max reward: 93.01098616304515, mean reward: 89.77342596505177\n",
      "episode: 282, current reward: 90.28024020265397, max reward: 92.9836164732344, mean reward: 90.13694276860373\n",
      "episode: 283, current reward: 92.31733380733854, max reward: 93.22302139824983, mean reward: 90.4311307693794\n",
      "episode: 284, current reward: 90.18171467572117, max reward: 93.13308169304322, mean reward: 90.22440474181396\n",
      "episode: 285, current reward: 90.95394174841475, max reward: 93.07824915407741, mean reward: 90.1676956203147\n",
      "episode: 286, current reward: 90.5284135388113, max reward: 93.22938154171679, mean reward: 90.47022859727984\n",
      "episode: 287, current reward: 90.43010447574521, max reward: 93.13572303640214, mean reward: 90.17239780491542\n",
      "episode: 288, current reward: 90.70698919192347, max reward: 92.9897717361719, mean reward: 90.01329917641253\n",
      "episode: 289, current reward: 90.52367115318225, max reward: 93.13375528139878, mean reward: 90.14013878632834\n",
      "episode: 290, current reward: 90.42861361791533, max reward: 93.31030383942726, mean reward: 90.57251151391898\n",
      "episode: 291, current reward: 90.29051877877433, max reward: 93.3529331120983, mean reward: 89.52463466502888\n",
      "episode: 292, current reward: 90.45503758194971, max reward: 93.3801509263108, mean reward: 90.47802845342407\n",
      "episode: 293, current reward: 87.5902700657282, max reward: 93.21097268449593, mean reward: 90.20916865680633\n",
      "episode: 294, current reward: 90.2529551806766, max reward: 93.15023778141409, mean reward: 90.35519021101625\n",
      "episode: 295, current reward: 90.29542292275248, max reward: 93.08424244395485, mean reward: 89.89861568441833\n",
      "episode: 296, current reward: 92.63825620085379, max reward: 93.0994188999506, mean reward: 90.11832166097281\n",
      "episode: 297, current reward: 90.0511107749695, max reward: 93.32014065118386, mean reward: 89.78718492491006\n",
      "episode: 298, current reward: 92.06093681863135, max reward: 89.90462677254594, mean reward: 89.52908050555565\n",
      "episode: 299, current reward: 91.01253943042158, max reward: 93.32293438393869, mean reward: 90.11037735157169\n",
      "episode: 300, current reward: 90.45804795447131, max reward: 89.88366454197637, mean reward: 89.53743941403759\n",
      "episode: 301, current reward: 90.32146209255245, max reward: 93.4331272796561, mean reward: 90.25911545541949\n",
      "episode: 302, current reward: 90.41554780874793, max reward: 93.46846138768572, mean reward: 90.67747470148062\n",
      "episode: 303, current reward: 89.07245244794777, max reward: 93.42526206634555, mean reward: 90.39241829854076\n",
      "episode: 304, current reward: 93.09991368098338, max reward: 93.49914954078397, mean reward: 90.62225693917411\n",
      "episode: 305, current reward: 90.55249906243071, max reward: 93.5829735066878, mean reward: 90.15413460451074\n",
      "episode: 306, current reward: 90.83352759208425, max reward: 93.63878020986756, mean reward: 90.55815994484944\n",
      "episode: 307, current reward: 90.65055232293061, max reward: 93.6697301162813, mean reward: 90.82381352057742\n",
      "episode: 308, current reward: 93.55975384130063, max reward: 93.65462828839716, mean reward: 92.04605591954756\n",
      "episode: 309, current reward: 90.43590288054742, max reward: 93.71042077268507, mean reward: 90.87695000949802\n",
      "episode: 310, current reward: 90.53119738286033, max reward: 93.79656592991472, mean reward: 90.3237760133213\n",
      "episode: 311, current reward: 90.71196390348352, max reward: 93.74835989457753, mean reward: 90.4295786851552\n",
      "episode: 312, current reward: 93.94428885980129, max reward: 93.75458664658045, mean reward: 91.81033959751994\n",
      "episode: 313, current reward: 90.81457324008974, max reward: 93.74597163251173, mean reward: 92.18589880933988\n",
      "episode: 314, current reward: 93.71242614131883, max reward: 93.75043064794623, mean reward: 91.97626680186347\n",
      "episode: 315, current reward: 93.81463626342321, max reward: 93.74312538552975, mean reward: 91.00806170354397\n",
      "episode: 316, current reward: 90.51381908054965, max reward: 93.73861146912233, mean reward: 91.82579887018127\n",
      "episode: 317, current reward: 93.90494376697613, max reward: 93.73372166958508, mean reward: 91.75827640485029\n",
      "episode: 318, current reward: 94.17913748051289, max reward: 93.71524948426439, mean reward: 92.34394730219167\n",
      "episode: 319, current reward: 93.8165858478786, max reward: 93.68438853615717, mean reward: 92.88936395346502\n",
      "episode: 320, current reward: 93.38961558767016, max reward: 93.69749310019715, mean reward: 92.61913749167182\n",
      "episode: 321, current reward: 90.75501176505348, max reward: 93.76211350105099, mean reward: 90.28033418639518\n",
      "episode: 322, current reward: 94.0031775868901, max reward: 93.68698449843097, mean reward: 92.91924320041934\n",
      "episode: 323, current reward: 90.5132090020181, max reward: 93.7347283792412, mean reward: 91.07957429837376\n",
      "episode: 324, current reward: 94.01224499550618, max reward: 93.69539025271956, mean reward: 92.05920850662956\n",
      "episode: 325, current reward: 93.95259058780155, max reward: 93.70733175807842, mean reward: 91.84861260741705\n",
      "episode: 326, current reward: 94.0747579046978, max reward: 93.68712536713632, mean reward: 92.77602309212935\n",
      "episode: 327, current reward: 93.87945428656911, max reward: 93.68211431591484, mean reward: 92.90160150337447\n",
      "episode: 328, current reward: 93.80415009959194, max reward: 93.76297347912845, mean reward: 90.4230853848454\n",
      "episode: 329, current reward: 90.59820000897741, max reward: 93.6710856472286, mean reward: 92.61306367707662\n",
      "episode: 330, current reward: 90.59450141110207, max reward: 93.67936798988444, mean reward: 92.61390218702303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 331, current reward: 94.01425703335073, max reward: 93.67487440789574, mean reward: 93.43827080896075\n",
      "episode: 332, current reward: 94.04282775975055, max reward: 93.67193964484801, mean reward: 93.43450551420287\n",
      "episode: 333, current reward: 93.01919970187086, max reward: 93.68184335889966, mean reward: 92.89917198662596\n",
      "episode: 334, current reward: 91.03496244746036, max reward: 93.6778208857416, mean reward: 92.68724011792521\n",
      "episode: 335, current reward: 90.86340768758481, max reward: 93.67560765316772, mean reward: 92.63579728449744\n",
      "episode: 336, current reward: 93.81541841470562, max reward: 93.66314213436084, mean reward: 93.42989697797222\n",
      "episode: 337, current reward: 93.91694830860067, max reward: 93.67677152405157, mean reward: 93.57009186610858\n",
      "episode: 338, current reward: 94.16391108450708, max reward: 93.67617323088577, mean reward: 92.62875437176433\n",
      "episode: 339, current reward: 93.86520387541617, max reward: 93.66358348647523, mean reward: 92.90589593592445\n",
      "episode: 340, current reward: 90.93366817577376, max reward: 93.66090919855043, mean reward: 93.30497844955867\n",
      "episode: 341, current reward: 93.87121324296835, max reward: 93.65345179081848, mean reward: 93.15632015686623\n",
      "episode: 342, current reward: 90.86180941177344, max reward: 93.65565482172948, mean reward: 93.04507712916703\n",
      "episode: 343, current reward: 93.90279338625454, max reward: 93.65366201007973, mean reward: 93.04206432998085\n",
      "episode: 344, current reward: 90.77048150439144, max reward: 93.65658496487701, mean reward: 93.47995365042011\n",
      "episode: 345, current reward: 93.85413581153578, max reward: 93.65467479112685, mean reward: 93.35585353430608\n",
      "episode: 346, current reward: 93.90950005676132, max reward: 93.6545119831484, mean reward: 93.41631347001197\n",
      "episode: 347, current reward: 94.01092407792288, max reward: 93.66301197227277, mean reward: 93.24970787922972\n",
      "episode: 348, current reward: 93.74881838324245, max reward: 93.66236207193413, mean reward: 93.43331987946878\n",
      "episode: 349, current reward: 93.84371223464046, max reward: 93.66563998881136, mean reward: 93.48530575678888\n",
      "episode: 350, current reward: 93.45047355459492, max reward: 93.67355511421809, mean reward: 93.25777701313389\n",
      "episode: 351, current reward: 93.97882318196619, max reward: 93.71365503127126, mean reward: 91.87473557980877\n",
      "episode: 352, current reward: 93.86364494123974, max reward: 93.67482910340841, mean reward: 93.43495911142507\n",
      "episode: 353, current reward: 93.23697690308661, max reward: 93.67683029874551, mean reward: 93.43435755766886\n",
      "episode: 354, current reward: 93.84430850401942, max reward: 93.70520620793675, mean reward: 93.56257241523082\n",
      "episode: 355, current reward: 93.86387136212645, max reward: 93.60220717682706, mean reward: 93.55149451556242\n",
      "episode: 356, current reward: 93.91578924135243, max reward: 93.69272419068673, mean reward: 93.55354397996987\n",
      "episode: 357, current reward: 93.81710054776745, max reward: 93.68653928486177, mean reward: 93.4351499247925\n",
      "episode: 358, current reward: 93.76281062097931, max reward: 93.61365980915424, mean reward: 93.55901733166249\n",
      "episode: 359, current reward: 93.85069875234439, max reward: 93.60356002396557, mean reward: 93.54912181207541\n",
      "episode: 360, current reward: 94.02885039850466, max reward: 93.61151890288575, mean reward: 93.55928758684479\n",
      "episode: 361, current reward: 93.90302719475872, max reward: 93.61431690796195, mean reward: 93.56112041977302\n",
      "episode: 362, current reward: 93.80351832173692, max reward: 93.62393118390513, mean reward: 93.57307077627587\n",
      "episode: 363, current reward: 93.77580880455574, max reward: 93.63095691074003, mean reward: 93.52237878729089\n",
      "episode: 364, current reward: 93.91864390636464, max reward: 93.63820766591563, mean reward: 93.57594705056641\n",
      "episode: 365, current reward: 93.88495278457299, max reward: 93.5553478597437, mean reward: 93.52428844848004\n",
      "episode: 366, current reward: 93.90052443225434, max reward: 93.5538418192213, mean reward: 93.5231883130656\n",
      "episode: 367, current reward: 93.87365023951338, max reward: 93.54411893631115, mean reward: 93.4625663427865\n",
      "episode: 368, current reward: 94.00157272604811, max reward: 93.54270341699787, mean reward: 93.51163862462539\n",
      "episode: 369, current reward: 93.78893790958946, max reward: 93.53785243029726, mean reward: 93.50307096537584\n",
      "episode: 370, current reward: 93.89341159147752, max reward: 93.45691384551455, mean reward: 93.41166889892402\n",
      "episode: 371, current reward: 93.81643084488479, max reward: 93.4410968048371, mean reward: 93.3648759358428\n",
      "episode: 372, current reward: 93.76246769665057, max reward: 93.36410742551706, mean reward: 93.31280085718149\n",
      "episode: 373, current reward: 93.69813391323397, max reward: 93.37040693489685, mean reward: 93.30820975255972\n",
      "episode: 374, current reward: 93.73780644591535, max reward: 93.36976883132328, mean reward: 93.31676096278642\n",
      "episode: 375, current reward: 93.4947242415833, max reward: 93.37724837093283, mean reward: 93.30937342206651\n",
      "episode: 376, current reward: 93.74636606199866, max reward: 93.33849099177786, mean reward: 93.25609085770992\n",
      "episode: 377, current reward: 93.51878255041447, max reward: 93.28421826974767, mean reward: 93.22870370483595\n",
      "episode: 378, current reward: 93.52205675570467, max reward: 93.19671542207402, mean reward: 93.14901737641263\n",
      "episode: 379, current reward: 93.52052870974332, max reward: 93.10176241592251, mean reward: 93.06240376862347\n",
      "episode: 380, current reward: 93.52193574483036, max reward: 93.11117943584586, mean reward: 93.05511147226254\n",
      "episode: 381, current reward: 93.53414815550998, max reward: 93.12672221210453, mean reward: 93.09352244620828\n",
      "episode: 382, current reward: 93.18479309612931, max reward: 93.15975484252745, mean reward: 93.0986908532923\n",
      "episode: 383, current reward: 93.45630352561108, max reward: 93.18800745454506, mean reward: 93.15282995615684\n",
      "episode: 384, current reward: 93.44219489214855, max reward: 93.01161747778471, mean reward: 92.96694166156273\n",
      "episode: 385, current reward: 93.49207094612177, max reward: 93.14736143373975, mean reward: 93.06584759623873\n",
      "episode: 386, current reward: 93.27126564847444, max reward: 93.1550321660168, mean reward: 93.10039747510386\n",
      "episode: 387, current reward: 93.56605536593206, max reward: 93.18317297694684, mean reward: 93.09169661516178\n",
      "episode: 388, current reward: 93.60236561683544, max reward: 93.19444371667399, mean reward: 93.11897216548113\n",
      "episode: 389, current reward: 93.4406534658819, max reward: 93.21732956657027, mean reward: 93.11339118176075\n",
      "episode: 390, current reward: 93.66247329631324, max reward: 93.11938441359192, mean reward: 93.05247064442028\n",
      "episode: 391, current reward: 93.68101328839398, max reward: 93.17405857028815, mean reward: 93.07831053232337\n",
      "episode: 392, current reward: 93.32577705245929, max reward: 93.18379869957128, mean reward: 93.10326721578946\n",
      "episode: 393, current reward: 93.42464772170455, max reward: 93.12290271032998, mean reward: 93.00405790928654\n",
      "episode: 394, current reward: 93.62973030531892, max reward: 93.13373234323272, mean reward: 93.04714795336292\n",
      "episode: 395, current reward: 93.34328577295715, max reward: 93.33846351112389, mean reward: 93.25545405707835\n",
      "episode: 396, current reward: 93.5284432620891, max reward: 93.10224120406176, mean reward: 93.0208802886012\n",
      "episode: 397, current reward: 93.38627289335557, max reward: 93.07478986428131, mean reward: 93.00035780882325\n",
      "episode: 398, current reward: 92.91442659677715, max reward: 93.04079846882554, mean reward: 92.9434997724255\n",
      "episode: 399, current reward: 93.25785642622031, max reward: 93.00028712719106, mean reward: 92.92239705279637\n",
      "episode: 400, current reward: 93.38619476323764, max reward: 93.1276676339943, mean reward: 93.02743222790794\n",
      "episode: 401, current reward: 92.96326239805047, max reward: 93.12458228936964, mean reward: 93.02429084758528\n",
      "episode: 402, current reward: 93.36084976611228, max reward: 92.91878390765301, mean reward: 92.81044199389096\n",
      "episode: 403, current reward: 93.26152848179538, max reward: 93.03419549244856, mean reward: 92.92987568349253\n",
      "episode: 404, current reward: 93.22917035487139, max reward: 93.16329039916693, mean reward: 93.04965439625371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405, current reward: 93.12177864879013, max reward: 92.99794326101375, mean reward: 92.89034317976575\n",
      "episode: 406, current reward: 93.47983674390464, max reward: 92.96439930089313, mean reward: 92.84747028088809\n",
      "episode: 407, current reward: 93.44185337660659, max reward: 92.88765284042826, mean reward: 92.7981903910325\n",
      "episode: 408, current reward: 93.14486790106514, max reward: 92.9600625540471, mean reward: 92.84414059624649\n",
      "episode: 409, current reward: 93.12859830854873, max reward: 92.84792645628391, mean reward: 92.7654898229603\n",
      "episode: 410, current reward: 93.30318805488864, max reward: 93.00211036718173, mean reward: 92.91247732234919\n",
      "episode: 411, current reward: 93.22592613849177, max reward: 92.91103593653813, mean reward: 92.83294532171718\n",
      "episode: 412, current reward: 93.26851345576215, max reward: 92.89704340069059, mean reward: 92.81000709302518\n",
      "episode: 413, current reward: 93.19662019700705, max reward: 92.75248314682545, mean reward: 92.66357433110646\n",
      "episode: 414, current reward: 93.05946981512935, max reward: 92.83138497621796, mean reward: 92.74745518000672\n",
      "episode: 415, current reward: 93.03897629125277, max reward: 93.11247223433243, mean reward: 92.94548826314234\n",
      "episode: 416, current reward: 93.53986253270149, max reward: 93.0890927163411, mean reward: 92.93678140589897\n",
      "episode: 417, current reward: 92.86354017350321, max reward: 92.96873480078698, mean reward: 92.78174018084242\n",
      "episode: 418, current reward: 93.23643041863268, max reward: 92.96644510919616, mean reward: 92.78091338372626\n",
      "episode: 419, current reward: 92.89181120475142, max reward: 92.87347445465667, mean reward: 92.68139156356625\n",
      "episode: 420, current reward: 92.63702767814581, max reward: 92.90980582783759, mean reward: 92.70269563066435\n",
      "episode: 421, current reward: 93.32451469183353, max reward: 92.99880957447373, mean reward: 92.86111878853349\n",
      "episode: 422, current reward: 93.21082822852212, max reward: 92.98507601205766, mean reward: 92.82479440654762\n",
      "episode: 423, current reward: 93.2000275357117, max reward: 92.8905066679758, mean reward: 92.70893140772365\n",
      "episode: 424, current reward: 93.48672489039774, max reward: 93.09905462117345, mean reward: 92.9348775010524\n",
      "episode: 425, current reward: 93.09819971303726, max reward: 92.97725624689177, mean reward: 92.7900561345661\n",
      "episode: 426, current reward: 92.85664102330237, max reward: 93.0566049037529, mean reward: 92.83908066725508\n",
      "episode: 427, current reward: 92.85801503903507, max reward: 92.92326277659457, mean reward: 92.74648340206087\n",
      "episode: 428, current reward: 92.99563864399883, max reward: 92.96670399730114, mean reward: 92.76462782359435\n",
      "episode: 429, current reward: 93.00894345908576, max reward: 93.04752425538496, mean reward: 92.87174149392186\n",
      "episode: 430, current reward: 93.17369607126074, max reward: 93.05468827021048, mean reward: 92.86959993798583\n",
      "episode: 431, current reward: 93.24174293189932, max reward: 93.01300496127448, mean reward: 92.85908536412744\n",
      "episode: 432, current reward: 93.07185466230567, max reward: 92.95646650034197, mean reward: 92.81837596009322\n",
      "episode: 433, current reward: 93.4073099070653, max reward: 92.78822132657263, mean reward: 92.6411342411729\n",
      "episode: 434, current reward: 93.27285585959004, max reward: 92.90437913628186, mean reward: 92.7981914078618\n",
      "episode: 435, current reward: 93.02276499598078, max reward: 92.87335806990676, mean reward: 92.77094942588096\n",
      "episode: 436, current reward: 92.94653748023293, max reward: 92.78269851664531, mean reward: 92.63717962607842\n",
      "episode: 437, current reward: 92.58804825274976, max reward: 92.99814099084384, mean reward: 92.8497858089239\n",
      "episode: 438, current reward: 92.89256203337337, max reward: 92.89316331875351, mean reward: 92.74992364619888\n",
      "episode: 439, current reward: 92.9557186338605, max reward: 93.03914978134232, mean reward: 92.80663947335866\n",
      "episode: 440, current reward: 93.26115165411707, max reward: 92.86788705193275, mean reward: 92.7610535121297\n",
      "episode: 441, current reward: 93.14949447906294, max reward: 92.90696762948707, mean reward: 92.77774318666718\n",
      "episode: 442, current reward: 93.28727832530757, max reward: 92.8354054678412, mean reward: 92.72368164542509\n",
      "episode: 443, current reward: 93.26631740106956, max reward: 92.89434829988738, mean reward: 92.76214609121318\n",
      "episode: 444, current reward: 93.06790681909956, max reward: 93.16159901378433, mean reward: 92.99374984902133\n",
      "episode: 445, current reward: 93.07381891690531, max reward: 93.27108989674642, mean reward: 93.01159426107861\n",
      "episode: 446, current reward: 93.72906820163996, max reward: 93.11337200699097, mean reward: 92.90115565792831\n",
      "episode: 447, current reward: 93.382343902277, max reward: 93.06991836271408, mean reward: 92.88254378244437\n",
      "episode: 448, current reward: 93.48433684158098, max reward: 93.03976544738714, mean reward: 92.88772413197705\n",
      "episode: 449, current reward: 93.28783172226262, max reward: 92.97444377195028, mean reward: 92.81056945830228\n",
      "episode: 450, current reward: 93.2153825577385, max reward: 93.09220256394401, mean reward: 92.92249547240122\n",
      "episode: 451, current reward: 93.10159085630617, max reward: 93.00514685798147, mean reward: 92.83223873452366\n",
      "episode: 452, current reward: 93.19096336387803, max reward: 92.96206237169878, mean reward: 92.85677546595154\n",
      "episode: 453, current reward: 93.31558412074993, max reward: 93.03026456543795, mean reward: 92.87814899162323\n",
      "episode: 454, current reward: 93.35579274427263, max reward: 93.06031089349592, mean reward: 92.89449326439384\n",
      "episode: 455, current reward: 93.29548051147962, max reward: 93.05017124968147, mean reward: 92.89017834817906\n",
      "episode: 456, current reward: 93.2408693188581, max reward: 92.99670142078136, mean reward: 92.85734287283984\n",
      "episode: 457, current reward: 93.2694582436751, max reward: 92.99914086827546, mean reward: 92.87519433832905\n",
      "episode: 458, current reward: 93.23726979489298, max reward: 93.08937643968426, mean reward: 92.8728229856282\n",
      "episode: 459, current reward: 93.20433971687979, max reward: 93.0483392487164, mean reward: 92.91775779449485\n",
      "episode: 460, current reward: 93.50894424140486, max reward: 93.06492028323882, mean reward: 92.92001137435186\n",
      "episode: 461, current reward: 93.38227936461215, max reward: 92.86104918516475, mean reward: 92.77311142176706\n",
      "episode: 462, current reward: 93.05619957930313, max reward: 93.17589865251107, mean reward: 92.94970392912903\n",
      "episode: 463, current reward: 93.37942361331183, max reward: 93.23868163671669, mean reward: 93.05379368411356\n",
      "episode: 464, current reward: 92.99207701917003, max reward: 93.20266768655705, mean reward: 93.00889655614988\n",
      "episode: 465, current reward: 93.2300674613636, max reward: 93.18797057680017, mean reward: 92.97478235835739\n",
      "episode: 466, current reward: 93.57414801609445, max reward: 93.44010820383747, mean reward: 93.13827346209891\n",
      "episode: 467, current reward: 93.50019543567709, max reward: 93.4717749444727, mean reward: 93.23575315075817\n",
      "episode: 468, current reward: 93.60093909991112, max reward: 93.50821588520435, mean reward: 93.25336336149019\n",
      "episode: 469, current reward: 93.6738318608203, max reward: 93.37840873024162, mean reward: 93.19096624482286\n",
      "episode: 470, current reward: 93.18019508277457, max reward: 93.16678214837849, mean reward: 92.99468053414928\n",
      "episode: 471, current reward: 93.35167007600518, max reward: 93.22276457502433, mean reward: 93.02221236542572\n",
      "episode: 472, current reward: 93.34070926472013, max reward: 93.12836777845769, mean reward: 92.91833643692854\n",
      "episode: 473, current reward: 93.48447630720024, max reward: 93.19185260134583, mean reward: 92.939059874969\n",
      "episode: 474, current reward: 93.27638050764025, max reward: 93.23413069165892, mean reward: 93.05898744627629\n",
      "episode: 475, current reward: 93.12774903725172, max reward: 93.15612392850745, mean reward: 92.98159413390874\n",
      "episode: 476, current reward: 92.99759212509207, max reward: 93.33683049581401, mean reward: 93.08228010644864\n",
      "episode: 477, current reward: 93.0641791669107, max reward: 93.23816232221283, mean reward: 93.02006922962268\n",
      "episode: 478, current reward: 93.43304256622234, max reward: 93.25934458307927, mean reward: 93.051521966109\n",
      "episode: 479, current reward: 93.58198658066073, max reward: 93.31301803890838, mean reward: 93.08754161202613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 480, current reward: 92.98666137772902, max reward: 93.38232026835136, mean reward: 93.11551749196724\n",
      "episode: 481, current reward: 92.89148431314982, max reward: 93.32976922790458, mean reward: 93.08702208586622\n",
      "episode: 482, current reward: 93.12957008027564, max reward: 93.26068654834208, mean reward: 93.04382475977359\n",
      "episode: 483, current reward: 93.2385391693501, max reward: 93.25465405859075, mean reward: 93.04121866263908\n",
      "episode: 484, current reward: 93.50955685635287, max reward: 93.0774029200683, mean reward: 92.8824791708951\n",
      "episode: 485, current reward: 93.43955922490106, max reward: 93.17894643500365, mean reward: 92.9786476426964\n",
      "episode: 486, current reward: 93.04143899422188, max reward: 93.13067548045673, mean reward: 92.95705515612727\n",
      "episode: 487, current reward: 93.35856787486951, max reward: 93.09025551646964, mean reward: 92.93401881635619\n",
      "episode: 488, current reward: 93.45258550529094, max reward: 93.17085798471516, mean reward: 93.01856001707161\n",
      "episode: 489, current reward: 93.56880360604463, max reward: 93.02035969783532, mean reward: 92.89204167203329\n",
      "episode: 490, current reward: 93.56749711268945, max reward: 93.016049513108, mean reward: 92.8805732429807\n",
      "episode: 491, current reward: 93.28742110919428, max reward: 93.04539425272455, mean reward: 92.89712680628543\n",
      "episode: 492, current reward: 92.88718975088204, max reward: 93.07352729314164, mean reward: 92.86748598175377\n",
      "episode: 493, current reward: 93.0005395603398, max reward: 93.05065546268033, mean reward: 92.90640191364328\n",
      "episode: 494, current reward: 93.21684361554794, max reward: 93.07478874729077, mean reward: 92.92268086856268\n",
      "episode: 495, current reward: 93.34766980403774, max reward: 93.0985705829184, mean reward: 92.9160868770231\n",
      "episode: 496, current reward: 93.51822885743724, max reward: 93.0650826451359, mean reward: 92.90359340721906\n",
      "episode: 497, current reward: 93.33110278679467, max reward: 92.95824451166877, mean reward: 92.7858457612726\n",
      "episode: 498, current reward: 92.89153681949256, max reward: 92.92942852363231, mean reward: 92.782634019318\n",
      "episode: 499, current reward: 93.39908161674467, max reward: 92.98932441494154, mean reward: 92.85898902754784\n",
      "episode: 500, current reward: 93.20659654780447, max reward: 92.98197999952737, mean reward: 92.86067942363057\n"
     ]
    }
   ],
   "source": [
    "def ddpg(episodes):\n",
    "    agent = Agent(state_size = state_size, action_size = action_size,\n",
    "              buffer_size = BUFFER_SIZE, batch_size = BATCH_SIZE,\n",
    "              gamma = GAMMA, tau = TAU)\n",
    "    reward_list = []\n",
    "    mean_reward = -20000\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            total_reward +=reward\n",
    "            state = next_state\n",
    "            \n",
    "        reward_list.append(total_reward)\n",
    "        agent.sd = max(agent.sd - 0.01, 0.1)\n",
    "        if total_reward > 50:\n",
    "            r = agent.test()\n",
    "            local_mean = np.mean(r)\n",
    "            print(f\"episode: {i+1}, current reward: {total_reward}, max reward: {np.max(r)}, mean reward: {local_mean}\")\n",
    "            if local_mean > mean_reward:\n",
    "                mean_reward = local_mean\n",
    "                agent.save()\n",
    "                print(\"Saved\")\n",
    "        else:\n",
    "            print(f\"episode: {i+1}, current reward: {total_reward}\")\n",
    "            \n",
    "            \n",
    "    return reward_list\n",
    "\n",
    "reward = ddpg(EPISODES)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (linear1): Linear(in_features=2, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (linear3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = torch.load(\"actor.pkl\")\n",
    "actor.to(device)\n",
    "\n",
    "#critic = torch.load(\"critic.pkl\")\n",
    "#critic.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_act(state):\n",
    "        state =  torch.tensor(state).to(device).float()\n",
    "        action = actor(state).cpu().data.numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max reward:  96.27186511366071\n",
      "mean reward:  92.64992503483003\n"
     ]
    }
   ],
   "source": [
    "reward = []\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    local_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = game_act(state)\n",
    "        state, r, done, _ = env.step(action)\n",
    "        local_reward += r\n",
    "    reward.append(local_reward)\n",
    "print(\"max reward: \", np.max(reward))\n",
    "print(\"mean reward: \", np.mean(reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
